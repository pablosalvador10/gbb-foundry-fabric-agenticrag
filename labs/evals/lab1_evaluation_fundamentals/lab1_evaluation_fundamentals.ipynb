{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Quality Assessment for Business Operations\n",
    "\n",
    "## Executive Overview\n",
    "This demonstration showcases how to evaluate AI-generated responses for customer-facing applications, ensuring consistent quality and accuracy in automated communications.\n",
    "\n",
    "## Key Business Outcomes\n",
    "- **Quality Assurance**: Systematic measurement of AI response quality\n",
    "- **Risk Mitigation**: Identify potential issues before customer impact\n",
    "- **Performance Optimization**: Data-driven improvements to AI systems\n",
    "- **Cost Management**: Optimize AI model selection based on quality vs. cost\n",
    "\n",
    "## Use Cases\n",
    "- Customer service chatbots\n",
    "- Automated response systems\n",
    "- Knowledge base queries\n",
    "- Support ticket routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Response Variability Analysis\n",
    "\n",
    "### The Challenge with AI Systems\n",
    "Unlike traditional software, AI models can generate different responses to identical inputs. This variability requires new quality assurance approaches for business-critical applications.\n",
    "\n",
    "**Business Impact**: Same customer inquiry might receive different responses, affecting service consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies imported successfully!\n",
      "Available demo prompts: ['factual', 'explanatory', 'analytical', 'instruction', 'service']\n",
      "ðŸ¢ Azure AI Foundry integration: ENABLED\n",
      "   âœ… Evaluation results will appear in the AI Foundry portal\n"
     ]
    }
   ],
   "source": [
    "# Let's start by importing our dependencies\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the path so we can import shared utilities\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from shared_utils.azure_clients import azure_manager\n",
    "from shared_utils.foundry_evaluation import foundry_runner\n",
    "from lab1_evaluation_fundamentals.utils.lab1_helpers import (\n",
    "    demonstrate_llm_variability, \n",
    "    print_evaluation_insights,\n",
    "    DEMO_PROMPTS\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Dependencies imported successfully!\")\n",
    "print(f\"Available demo prompts: {list(DEMO_PROMPTS.keys())}\")\n",
    "\n",
    "# Check AI Foundry integration status\n",
    "foundry_status = foundry_runner.get_status_info()\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    print(\"ðŸ¢ Azure AI Foundry integration: ENABLED\")\n",
    "    print(\"   âœ… Evaluation results will appear in the AI Foundry portal\")\n",
    "else:\n",
    "    print(\"ðŸ”§ Azure AI Foundry integration: DISABLED\")\n",
    "    print(\"   â„¹ï¸ To enable portal integration, add to .env:\")\n",
    "    print(\"      AZURE_AI_FOUNDRY_PROJECT_NAME, AZURE_AI_FOUNDRY_ENDPOINT, AZURE_AI_FOUNDRY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Same Prompt, Different Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š BUSINESS ANALYSIS: AI Response Variability\n",
      "Testing consistency of customer service responses...\n",
      "\n",
      "Running the same prompt 3 times to show variability...\n",
      "Prompt: What are the standard delivery timeframes for domestic overnight shipping?\n",
      "--------------------------------------------------------------------------------\n",
      "Run 1:\n",
      "Domestic overnight shipping typically guarantees delivery by the next business day. The exact delivery timeframe can vary depending on the carrier and the specific service chosen, but generally, it aims to ensure arrival by the end of the next business day. Some services offer delivery as early as 8:00 AM, 10:30 AM, or by noon, while others may promise delivery by the end of the day, usually around 3:00 PM to 8:00 PM. It's important to check with the specific carrier, such as FedEx, UPS, or USPS, for their exact delivery commitments and any conditions that may apply.\n",
      "--------------------------------------------------------------------------------\n",
      "Run 1:\n",
      "Domestic overnight shipping typically guarantees delivery by the next business day. The exact delivery timeframe can vary depending on the carrier and the specific service chosen, but generally, it aims to ensure arrival by the end of the next business day. Some services offer delivery as early as 8:00 AM, 10:30 AM, or by noon, while others may promise delivery by the end of the day, usually around 3:00 PM to 8:00 PM. It's important to check with the specific carrier, such as FedEx, UPS, or USPS, for their exact delivery commitments and any conditions that may apply.\n",
      "--------------------------------------------------------------------------------\n",
      "Run 2:\n",
      "The standard delivery timeframes for domestic overnight shipping typically involve delivery by the next business day. This can vary slightly depending on the service provider and the specific locations involved. Here are some general guidelines:\n",
      "\n",
      "1. **Major Carriers:** Companies like FedEx, UPS, and USPS offer overnight shipping services that usually guarantee delivery by the next business day. FedEx and UPS often offer several options, such as delivery by 10:30 AM, noon, or end of the day. USPS offers Priority Mail Express, which usually guarantees next-day delivery by 3 PM or 6 PM to most locations.\n",
      "\n",
      "2. **Cut-off Times:** Each carrier has specific cut-off times for overnight shipping, which can vary by location. Typically, this means packages need\n",
      "--------------------------------------------------------------------------------\n",
      "Run 2:\n",
      "The standard delivery timeframes for domestic overnight shipping typically involve delivery by the next business day. This can vary slightly depending on the service provider and the specific locations involved. Here are some general guidelines:\n",
      "\n",
      "1. **Major Carriers:** Companies like FedEx, UPS, and USPS offer overnight shipping services that usually guarantee delivery by the next business day. FedEx and UPS often offer several options, such as delivery by 10:30 AM, noon, or end of the day. USPS offers Priority Mail Express, which usually guarantees next-day delivery by 3 PM or 6 PM to most locations.\n",
      "\n",
      "2. **Cut-off Times:** Each carrier has specific cut-off times for overnight shipping, which can vary by location. Typically, this means packages need\n",
      "--------------------------------------------------------------------------------\n",
      "Run 3:\n",
      "Standard delivery timeframes for domestic overnight shipping typically involve the package being delivered by the next business day. The specific delivery time can vary based on the service provider and the destination. Hereâ€™s a general breakdown:\n",
      "\n",
      "1. **FedEx Overnight Services:**\n",
      "   - **FedEx First Overnight:** Delivery as early as 8:00 AM in most areas.\n",
      "   - **FedEx Priority Overnight:** Delivery by 10:30 AM to most U.S. addresses; noon or 4:30 PM in remote areas.\n",
      "   - **FedEx Standard Overnight:** Delivery by 3:00 PM to most U.S. addresses; 4:30 PM to rural areas.\n",
      "\n",
      "2. **UPS Overnight Services:**\n",
      "   - **UPS Next Day Air\n",
      "--------------------------------------------------------------------------------\n",
      "Run 3:\n",
      "Standard delivery timeframes for domestic overnight shipping typically involve the package being delivered by the next business day. The specific delivery time can vary based on the service provider and the destination. Hereâ€™s a general breakdown:\n",
      "\n",
      "1. **FedEx Overnight Services:**\n",
      "   - **FedEx First Overnight:** Delivery as early as 8:00 AM in most areas.\n",
      "   - **FedEx Priority Overnight:** Delivery by 10:30 AM to most U.S. addresses; noon or 4:30 PM in remote areas.\n",
      "   - **FedEx Standard Overnight:** Delivery by 3:00 PM to most U.S. addresses; 4:30 PM to rural areas.\n",
      "\n",
      "2. **UPS Overnight Services:**\n",
      "   - **UPS Next Day Air\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ’¼ BUSINESS INSIGHT:\n",
      "Each response varies despite identical customer inquiries.\n",
      "This demonstrates the need for systematic quality evaluation in customer-facing AI systems.\n",
      "\n",
      "ðŸ’¼ BUSINESS INSIGHT:\n",
      "Each response varies despite identical customer inquiries.\n",
      "This demonstrates the need for systematic quality evaluation in customer-facing AI systems.\n"
     ]
    }
   ],
   "source": [
    "# Get Azure OpenAI client\n",
    "client = azure_manager.get_openai_client()\n",
    "deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "\n",
    "# Test AI response consistency for customer service scenarios\n",
    "test_prompt = DEMO_PROMPTS['factual']\n",
    "\n",
    "print(\"ðŸ“Š BUSINESS ANALYSIS: AI Response Variability\")\n",
    "print(\"Testing consistency of customer service responses...\\n\")\n",
    "\n",
    "responses = demonstrate_llm_variability(\n",
    "    client=client,\n",
    "    deployment_name=deployment_name,\n",
    "    prompt=test_prompt,\n",
    "    num_runs=3\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ’¼ BUSINESS INSIGHT:\")\n",
    "print(\"Each response varies despite identical customer inquiries.\")\n",
    "print(\"This demonstrates the need for systematic quality evaluation in customer-facing AI systems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Risks of Inconsistent AI Performance\n",
    "\n",
    "**Customer Experience Impact**:\n",
    "- Inconsistent service responses affecting customer satisfaction\n",
    "- Incorrect shipping information leading to delivery issues\n",
    "- Inaccurate cost estimates impacting customer trust\n",
    "\n",
    "**Operational Risks**:\n",
    "- Regulatory compliance issues with automated responses\n",
    "- Increased support ticket volume due to AI errors\n",
    "- Brand reputation damage from poor automated interactions\n",
    "\n",
    "**Financial Impact**:\n",
    "- Customer churn from poor service experiences\n",
    "- Increased operational costs from manual intervention\n",
    "- Potential liability from incorrect automated guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assessment Framework\n",
    "\n",
    "### Key Performance Indicators\n",
    "\n",
    "**Relevance**: Does the AI directly address the customer's inquiry?\n",
    "- Critical for customer satisfaction and resolution rates\n",
    "\n",
    "**Coherence**: Is the response clear and professionally structured?\n",
    "- Impacts customer comprehension and brand perception\n",
    "\n",
    "**Accuracy**: Are facts and procedures correctly stated?\n",
    "- Essential for operational compliance and customer trust\n",
    "\n",
    "**Consistency**: Does the AI provide uniform responses to similar queries?\n",
    "- Ensures reliable service standards across all customer interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Analysis Dataset: 16 customer interactions\n",
      "\n",
      "Customer Query: What are your delivery options for overnight shipping?\n",
      "AI Response: We offer overnight delivery with guaranteed next-business-day delivery by 10:30 AM or 3:00 PM, depending on your location and service selected. Rates vary by package weight and destination zone.\n",
      "Reference Information: Company provides multiple shipping service levels including overnight express, 2-day, and ground shi...\n",
      "Expected Answer: Overnight shipping options include Express Overnight (delivery by 10:30 AM next business day) and Standard Overnight (delivery by 3:00 PM next business day). Pricing depends on package weight, dimensions, and destination zone.\n",
      "\n",
      "ðŸ“Š Data Structure:\n",
      "- Customer inquiries with AI-generated responses\n",
      "- Reference materials used by AI system\n",
      "- Expected/ideal responses for comparison\n"
     ]
    }
   ],
   "source": [
    "# Load customer service interaction data for analysis\n",
    "from shared_utils.evaluation_helpers import load_evaluation_data\n",
    "\n",
    "# Load sample customer inquiries and AI responses\n",
    "sample_data = load_evaluation_data('data/sample_qa_pairs.jsonl')\n",
    "\n",
    "print(f\"ðŸ“‹ Analysis Dataset: {len(sample_data)} customer interactions\")\n",
    "\n",
    "# Display sample interaction\n",
    "example = sample_data[0]\n",
    "print(f\"\\nCustomer Query: {example['query']}\")\n",
    "print(f\"AI Response: {example['response']}\")\n",
    "print(f\"Reference Information: {example['context'][:100]}...\")\n",
    "print(f\"Expected Answer: {example['ground_truth']}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Data Structure:\")\n",
    "print(\"- Customer inquiries with AI-generated responses\")\n",
    "print(\"- Reference materials used by AI system\")\n",
    "print(\"- Expected/ideal responses for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Business Metrics\n",
    "\n",
    "**Compliance & Risk**:\n",
    "- Content appropriateness for customer communications\n",
    "- Adherence to company policies and procedures\n",
    "- Regulatory compliance verification\n",
    "\n",
    "**Operational Efficiency**:\n",
    "- Response time impact on customer experience\n",
    "- Cost per interaction analysis\n",
    "- System throughput during peak periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Assessment Implementation\n",
    "\n",
    "Systematic evaluation of AI responses using industry-standard metrics and Azure AI platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize Assessment Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š QUALITY ASSESSMENT PLATFORM\n",
      "========================================\n",
      "âœ… Azure AI Platform: CONNECTED\n",
      "   Enterprise-grade evaluation framework active\n",
      "   Dashboard: https://ai.azure.com\n",
      "\n",
      "ðŸ”§ Assessment Mode: enabled\n",
      "   Ready to analyze AI response quality\n"
     ]
    }
   ],
   "source": [
    "# Initialize quality assessment platform\n",
    "foundry_status = foundry_runner.get_status_info()\n",
    "print(\"ðŸ“Š QUALITY ASSESSMENT PLATFORM\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    print(\"âœ… Azure AI Platform: CONNECTED\")\n",
    "    print(\"   Enterprise-grade evaluation framework active\")\n",
    "    print(\"   Dashboard: https://ai.azure.com\")\n",
    "else:\n",
    "    print(\"ðŸ“‹ Local Assessment Mode: ACTIVE\")\n",
    "    print(\"   Full evaluation capabilities available\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Assessment Mode: {foundry_status['portal_integration']}\")\n",
    "print(\"   Ready to analyze AI response quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Assessment framework initialized\n",
      "Using model: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Load assessment components\n",
    "from azure.ai.evaluation import evaluate, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Configure assessment models\n",
    "model_config = azure_manager.get_model_config()\n",
    "\n",
    "print(\"âœ… Assessment framework initialized\")\n",
    "print(f\"Using model: {model_config['azure_deployment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configure Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize quality assessment metrics\n",
    "print(\"âš™ï¸ Configuring assessment metrics...\")\n",
    "\n",
    "try:\n",
    "    relevance_evaluator = RelevanceEvaluator(model_config=model_config)\n",
    "    print(\"âœ… Relevance assessment ready\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Relevance metric unavailable: {e}\")\n",
    "    relevance_evaluator = None\n",
    "\n",
    "try:\n",
    "    coherence_evaluator = CoherenceEvaluator(model_config=model_config)\n",
    "    print(\"âœ… Coherence assessment ready\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Coherence metric unavailable: {e}\")\n",
    "    coherence_evaluator = None\n",
    "\n",
    "try:\n",
    "    fluency_evaluator = FluencyEvaluator(model_config=model_config)\n",
    "    print(\"âœ… Fluency assessment ready\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Fluency metric unavailable: {e}\")\n",
    "    fluency_evaluator = None\n",
    "\n",
    "try:\n",
    "    groundedness_evaluator = GroundednessEvaluator(model_config=model_config)\n",
    "    print(\"âœ… Accuracy assessment ready\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Accuracy metric unavailable: {e}\")\n",
    "    groundedness_evaluator = None\n",
    "\n",
    "# Configure available metrics\n",
    "evaluators = {}\n",
    "if relevance_evaluator:\n",
    "    evaluators[\"relevance\"] = relevance_evaluator\n",
    "if coherence_evaluator:\n",
    "    evaluators[\"coherence\"] = coherence_evaluator\n",
    "if fluency_evaluator:\n",
    "    evaluators[\"fluency\"] = fluency_evaluator\n",
    "if groundedness_evaluator:\n",
    "    evaluators[\"groundedness\"] = groundedness_evaluator\n",
    "\n",
    "print(f\"\\n\udccb Active metrics: {len(evaluators)} - {list(evaluators.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Select Assessment Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Running evaluation on 3 examples...\n",
      "\n",
      "Example 1:\n",
      "  Query: What is the capital of France?\n",
      "  Response: The capital of France is Paris....\n",
      "  Has context: Yes\n",
      "\n",
      "Example 2:\n",
      "  Query: How do you calculate the area of a circle?\n",
      "  Response: The area of a circle is calculated using the formula A = Ï€rÂ², where r is the radius of the circle....\n",
      "  Has context: Yes\n",
      "\n",
      "Example 3:\n",
      "  Query: What is machine learning?\n",
      "  Response: Machine learning is a subset of artificial intelligence that enables computers to learn and make dec...\n",
      "  Has context: Yes\n"
     ]
    }
   ],
   "source": [
    "# Select representative customer interactions for assessment\n",
    "evaluation_data = sample_data[:3]  # Sample of 3 interactions\n",
    "\n",
    "print(f\"ðŸ“Š Assessing {len(evaluation_data)} customer interactions...\")\n",
    "\n",
    "# Display sample interactions\n",
    "for i, item in enumerate(evaluation_data):\n",
    "    print(f\"\\nInteraction {i+1}:\")\n",
    "    print(f\"  Customer Query: {item['query']}\")\n",
    "    print(f\"  AI Response: {item['response'][:100]}...\")\n",
    "    print(f\"  Reference Data: {'Available' if item.get('context') else 'Not provided'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Execute Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running evaluation...\n",
      "This may take a few minutes as we call the Azure OpenAI service for each metric...\n",
      "âŒ Evaluation failed: (UserError) Unable to load data from './data/lab1_basic_evaluation.json'. Supported formats are JSONL and CSV. Detailed error: Expected object or value.\n",
      "\n",
      "ðŸ”§ Troubleshooting tips:\n",
      "1. Check your Azure OpenAI credentials in .env\n",
      "2. Verify your deployment name is correct\n",
      "3. Ensure you have quota available in your Azure subscription\n",
      "4. Check if your endpoint is accessible\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import evaluate, AzureAIProject\n",
    "\n",
    "# Execute comprehensive quality assessment\n",
    "print(\"ðŸ” Analyzing AI response quality...\")\n",
    "print(\"Processing customer interactions against business standards...\")\n",
    "\n",
    "try:\n",
    "    client = AIProjectClient(\n",
    "        endpoint=os.environ[\"AZURE_AI_FOUNDRY_ENDPOINT\"],\n",
    "        credential=DefaultAzureCredential()\n",
    "    )\n",
    "\n",
    "    # Use the enhanced foundry evaluation runner\n",
    "    # results = foundry_runner.run_evaluation(\n",
    "    #     data=evaluation_data,\n",
    "    #     evaluators=evaluators,\n",
    "    #     run_name=f\"Lab 1 Basic Evaluation - {len(evaluation_data)} items\",\n",
    "    #     description=f\"Foundational evaluation with {len(evaluators)} quality metrics\"\n",
    "    # )\n",
    "    proj = AzureAIProject(\n",
    "        subscription_id=os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        resource_group_name=os.environ[\"AZURE_RESOURCE_GROUP_NAME\"],\n",
    "        project_name=os.environ[\"AZURE_AI_FOUNDRY_PROJECT_NAME\"],\n",
    "    )\n",
    "    results = evaluate(\n",
    "        data='data/sample_qa_pairs.jsonl',\n",
    "        evaluators=evaluators,\n",
    "        project=proj,\n",
    "    )\n",
    "    print(\"âœ… Quality assessment completed successfully!\")\n",
    "    print(f\"ðŸ“Š Analyzed {len(evaluation_data)} customer interactions\")\n",
    "    print(\"ðŸ“ˆ Business metrics calculated and ready for review\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Assessment encountered an issue: {e}\")\n",
    "    print(\"\\nðŸ”§ System check recommendations:\")\n",
    "    print(\"1. Verify Azure AI platform connectivity\")\n",
    "    print(\"2. Confirm service deployment status\")\n",
    "    print(\"3. Check resource availability and quotas\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Business Intelligence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.30.1\n",
      "ðŸ“ Loading results from data/lab1_basic_evaluation.json\n",
      "âœ… Results loaded successfully!\n",
      "ðŸ” EVALUATION INSIGHTS\n",
      "==================================================\n",
      "ðŸ“Š Overall Metrics:\n",
      "  â€¢ relevance.relevance: 4.000\n",
      "    â†’ Excellent - Response directly addresses the question\n",
      "  â€¢ relevance.gpt_relevance: 4.000\n",
      "    â†’ Excellent - Response directly addresses the question\n",
      "  â€¢ relevance.relevance_threshold: 3.000\n",
      "    â†’ Good - Response is mostly relevant with minor issues\n",
      "  â€¢ coherence.coherence: 4.000\n",
      "    â†’ Excellent - Response is very clear and well-structured\n",
      "  â€¢ coherence.gpt_coherence: 4.000\n",
      "    â†’ Excellent - Response is very clear and well-structured\n",
      "  â€¢ coherence.coherence_threshold: 3.000\n",
      "    â†’ Good - Response is mostly coherent with good flow\n",
      "  â€¢ fluency.fluency: 3.667\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ fluency.gpt_fluency: 3.667\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ fluency.fluency_threshold: 3.000\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ groundedness.groundedness: 4.000\n",
      "    â†’ Excellent - Response is fully supported by the provided context\n",
      "  â€¢ groundedness.gpt_groundedness: 4.000\n",
      "    â†’ Excellent - Response is fully supported by the provided context\n",
      "  â€¢ groundedness.groundedness_threshold: 3.000\n",
      "    â†’ Good - Response is mostly grounded in the context\n",
      "  â€¢ relevance.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response doesn't adequately address the question\n",
      "  â€¢ coherence.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response lacks clear structure or logical flow\n",
      "  â€¢ fluency.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response has significant language or grammar problems\n",
      "  â€¢ groundedness.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response contains claims not supported by the context\n",
      "\n",
      "ðŸ“ˆ Individual Results (showing first 3 of 3):\n",
      "\n",
      "  Result 1:\n",
      "\n",
      "  Result 2:\n",
      "\n",
      "  Result 3:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import ipykernel\n",
    "\n",
    "ipykernel.__version__\n",
    "print(ipykernel.__version__)\n",
    "results_file = \"labs/evals/lab1_evaluation_fundamentals/data/lab1_basic_evaluation.json\"\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    print(f\"ðŸ“Š Loading assessment results from {results_file}\")\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(\"âœ… Business intelligence data loaded\")\n",
    "\n",
    "    # Generate executive insights\n",
    "    print_evaluation_insights(results)\n",
    "\n",
    "else:\n",
    "    print(f\"ðŸ“‹ Assessment data not available at {results_file}\")\n",
    "    print(\"Execute the quality assessment in the previous step to generate results.\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary & Business Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š UNDERSTANDING YOUR EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "ðŸŽ¯ Overall Average Scores:\n",
      "\n",
      "RELEVANCE: 4.000\n",
      "  ðŸ“ˆ EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "COHERENCE: 4.000\n",
      "  ðŸ“ˆ EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "FLUENCY: 3.667\n",
      "  ðŸ“Š GOOD - Solid performance with room for improvement\n",
      "\n",
      "GROUNDEDNESS: 4.000\n",
      "  ðŸ“ˆ EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "ðŸ“‹ Individual Results (3 examples):\n",
      "========================================\n",
      "\n",
      "Example 1:\n",
      "  Query: What is the capital of France?\n",
      "  Scores:\n",
      "    â€¢ Relevance: 4.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 3.0/5.0\n",
      "    â€¢ Groundedness: 4.0/5.0\n",
      "  Key Insights:\n",
      "    â€¢ Relevance: The response directly and accurately answers the query by stating that Paris is ...\n",
      "    â€¢ Coherence: The response is fully coherent, directly answers the question, and is logically ...\n",
      "\n",
      "Example 2:\n",
      "  Query: How do you calculate the area of a circle?\n",
      "  Scores:\n",
      "    â€¢ Relevance: 4.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 4.0/5.0\n",
      "    â€¢ Groundedness: 5.0/5.0\n",
      "  Key Insights:\n",
      "    â€¢ Relevance: The response directly and accurately provides the formula for calculating the ar...\n",
      "    â€¢ Coherence: The response is coherent, logically organized, and clearly explains how to calcu...\n",
      "\n",
      "Example 3:\n",
      "  Query: What is machine learning?\n",
      "  Scores:\n",
      "    â€¢ Relevance: 4.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 4.0/5.0\n",
      "    â€¢ Groundedness: 3.0/5.0\n",
      "  Key Insights:\n",
      "    â€¢ Relevance: The response directly and accurately defines machine learning, specifying its re...\n",
      "    â€¢ Coherence: The response is coherent, logically organized, and clearly explains what machine...\n",
      "\n",
      "ðŸ’¡ Key Insights:\n",
      "â€¢ Scores are on a 1-5 scale (5 being the best)\n",
      "â€¢ Consistency across metrics indicates well-balanced responses\n",
      "â€¢ Low groundedness might indicate hallucination issues\n",
      "â€¢ Low relevance suggests the model isn't understanding the query well\n"
     ]
    }
   ],
   "source": [
    "if results and 'metrics' in results:\n",
    "    print(\"ðŸ’¼ BUSINESS PERFORMANCE DASHBOARD\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    metrics = results['metrics']\n",
    "    print(\"\\nðŸ“Š Key Performance Indicators:\")\n",
    "\n",
    "    # Extract primary business metrics\n",
    "    main_metrics = {}\n",
    "    for metric_name, score in metrics.items():\n",
    "        if not any(x in metric_name for x in ['threshold', 'binary_aggregate', 'gpt_']):\n",
    "            base_name = metric_name.split('.')[0]\n",
    "            if base_name not in main_metrics:\n",
    "                main_metrics[base_name] = score\n",
    "\n",
    "    for metric_name, score in main_metrics.items():\n",
    "        print(f\"\\n{metric_name.upper()}: {score:.3f}/5.0\")\n",
    "\n",
    "        if score >= 4.0:\n",
    "            print(\"  âœ… EXCEEDS STANDARDS - Industry-leading performance\")\n",
    "        elif score >= 3.0:\n",
    "            print(\"  âœ“ MEETS STANDARDS - Acceptable business performance\")\n",
    "        elif score >= 2.0:\n",
    "            print(\"  âš ï¸ NEEDS ATTENTION - Below target performance\")\n",
    "        else:\n",
    "            print(\"  ðŸ”´ CRITICAL - Immediate improvement required\")\n",
    "\n",
    "    # Display individual interaction analysis\n",
    "    if 'rows' in results:\n",
    "        print(f\"\\nðŸ“‹ Customer Interaction Analysis ({len(results['rows'])} interactions):\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for i, row in enumerate(results['rows']):\n",
    "            print(f\"\\nCustomer Interaction {i+1}:\")\n",
    "\n",
    "            # Show customer query\n",
    "            query = row.get('inputs.query', 'Query not available')\n",
    "            print(f\"  Customer Request: {query}\")\n",
    "\n",
    "            # Extract performance scores\n",
    "            scores = {}\n",
    "            for key, value in row.items():\n",
    "                if key.startswith('outputs.') and key.endswith('.relevance'):\n",
    "                    scores['Response Relevance'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.coherence'):\n",
    "                    scores['Clarity & Structure'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.fluency'):\n",
    "                    scores['Professional Communication'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.groundedness'):\n",
    "                    scores['Factual Accuracy'] = value\n",
    "\n",
    "            print(\"  Performance Metrics:\")\n",
    "            for metric, score in scores.items():\n",
    "                status = \"âœ…\" if score >= 4.0 else \"âš ï¸\" if score >= 3.0 else \"ðŸ”´\"\n",
    "                print(f\"    {status} {metric}: {score:.1f}/5.0\")\n",
    "\n",
    "    print(\"\\nðŸ’¡ Business Insights:\")\n",
    "    print(\"â€¢ 5.0 = Exceptional customer service quality\")\n",
    "    print(\"â€¢ 4.0+ = Meets premium service standards\")\n",
    "    print(\"â€¢ 3.0+ = Acceptable for standard operations\")\n",
    "    print(\"â€¢ <3.0 = Requires immediate attention\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nðŸ“Š FRAMEWORK UNDERSTANDING\")\n",
    "    print(\"Key business capabilities demonstrated:\")\n",
    "    print(\"âœ… Enterprise AI quality assessment platform\")\n",
    "    print(\"âœ… Systematic customer service evaluation\")\n",
    "    print(\"âœ… Business performance metric interpretation\")\n",
    "    print(\"âœ… Risk mitigation through proactive monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” EVALUATION INSIGHTS\n",
      "==================================================\n",
      "ðŸ“Š Overall Metrics:\n",
      "  â€¢ relevance.relevance: 4.000\n",
      "    â†’ Excellent - Response directly addresses the question\n",
      "  â€¢ relevance.gpt_relevance: 4.000\n",
      "    â†’ Excellent - Response directly addresses the question\n",
      "  â€¢ relevance.relevance_threshold: 3.000\n",
      "    â†’ Good - Response is mostly relevant with minor issues\n",
      "  â€¢ coherence.coherence: 4.000\n",
      "    â†’ Excellent - Response is very clear and well-structured\n",
      "  â€¢ coherence.gpt_coherence: 4.000\n",
      "    â†’ Excellent - Response is very clear and well-structured\n",
      "  â€¢ coherence.coherence_threshold: 3.000\n",
      "    â†’ Good - Response is mostly coherent with good flow\n",
      "  â€¢ fluency.fluency: 3.667\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ fluency.gpt_fluency: 3.667\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ fluency.fluency_threshold: 3.000\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ groundedness.groundedness: 4.000\n",
      "    â†’ Excellent - Response is fully supported by the provided context\n",
      "  â€¢ groundedness.gpt_groundedness: 4.000\n",
      "    â†’ Excellent - Response is fully supported by the provided context\n",
      "  â€¢ groundedness.groundedness_threshold: 3.000\n",
      "    â†’ Good - Response is mostly grounded in the context\n",
      "  â€¢ relevance.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response doesn't adequately address the question\n",
      "  â€¢ coherence.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response lacks clear structure or logical flow\n",
      "  â€¢ fluency.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response has significant language or grammar problems\n",
      "  â€¢ groundedness.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response contains claims not supported by the context\n",
      "\n",
      "ðŸ“ˆ Individual Results (showing first 3 of 3):\n",
      "\n",
      "  Result 1:\n",
      "\n",
      "  Result 2:\n",
      "\n",
      "  Result 3:\n",
      "âœ… Results saved to: /Users/jinle/Repos/_Demos/llm-evaluations-workshop/lab1_evaluation_fundamentals/utils/../data/lab1_basic_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    # Use our helper function to display insights\n",
    "    print_evaluation_insights(results)\n",
    "    \n",
    "    # Save results for later analysis\n",
    "    from lab1_evaluation_fundamentals.utils.lab1_helpers import save_lab1_results\n",
    "    save_lab1_results(results, \"lab1_basic_evaluation.json\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No results to display - evaluation may have failed.\")\n",
    "    print(\"Don't worry! This is common in workshop environments.\")\n",
    "    print(\"The key learning is understanding the evaluation process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª EXPERIMENT: Comparing Response Quality\n",
      "Let's see how different response qualities are scored...\n",
      "\n",
      "Test Case 1: High - Direct, accurate, relevant\n",
      "Response: The capital of France is Paris. Paris is located in the north-central part of France and is the country's largest city.\n",
      "------------------------------------------------------------\n",
      "Test Case 2: Low - Doesn't answer the question\n",
      "Response: Well, France has many beautiful cities, and I think you might be interested in learning about French culture and cuisine. The Eiffel Tower is very famous.\n",
      "------------------------------------------------------------\n",
      "Test Case 3: Medium - Correct but adds irrelevant info\n",
      "Response: The capital is Paris and also France has a population of 67 million people living in cities like Lyon and Marseille which are also important economic centers.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate response quality variations with business-relevant scenarios\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What are your delivery options for overnight shipping?\",\n",
    "        \"response\": \"We offer overnight delivery with guaranteed next-business-day delivery by 10:30 AM or 3:00 PM, depending on your location and service selected. Rates vary by package weight and destination.\",\n",
    "        \"context\": \"Company offers various delivery timeframes including overnight, 2-day, and ground shipping options with different pricing tiers.\",\n",
    "        \"expected_quality\": \"High - Direct, complete, actionable\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are your delivery options for overnight shipping?\",\n",
    "        \"response\": \"We have many shipping services available and our company has been in business for many decades providing reliable service to customers worldwide with various options.\",\n",
    "        \"context\": \"Company offers various delivery timeframes including overnight, 2-day, and ground shipping options with different pricing tiers.\",\n",
    "        \"expected_quality\": \"Low - Vague, doesn't address specific question\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are your delivery options for overnight shipping?\",\n",
    "        \"response\": \"Overnight shipping is available with next-day delivery. We also provide tracking services, insurance options, and have warehouses in major cities for faster processing.\",\n",
    "        \"context\": \"Company offers various delivery timeframes including overnight, 2-day, and ground shipping options with different pricing tiers.\",\n",
    "        \"expected_quality\": \"Medium - Answers question but adds tangential information\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ðŸ“Š QUALITY COMPARISON ANALYSIS\")\n",
    "print(\"Evaluating different response quality levels for business impact...\\n\")\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    print(f\"Scenario {i+1}: {case['expected_quality']}\")\n",
    "    print(f\"AI Response: {case['response']}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Evaluating different response qualities...\n",
      "\n",
      "ðŸ¢ Running evaluation through Azure AI Foundry...\n",
      "   âœ… Results will appear in the AI Foundry portal\n",
      "ðŸ¢ Running evaluation with Azure AI Foundry integration...\n",
      "ðŸ“¤ Uploading dataset to AI Foundry: evaluation_dataset_756063988765806918\n",
      "âœ… Dataset uploaded successfully: azureai://accounts/aifoundry825233136833-resource/projects/aifoundry825233136833/data/evaluation_dataset_756063988765806918/versions/1.0\n",
      "ðŸ“Š Running evaluation locally (AI Foundry evaluation API not yet available)\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 1.27 seconds. Estimated time for incomplete lines: 2.54 seconds.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 0.69 seconds. Estimated time for incomplete lines: 0.69 seconds.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 0.5 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:42 -0500 6285193216 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 1.71 seconds. Estimated time for incomplete lines: 3.42 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 1.87 seconds. Estimated time for incomplete lines: 3.74 seconds.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 0.96 seconds.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 0.66 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 1.97 seconds. Estimated time for incomplete lines: 3.94 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 1.05 seconds. Estimated time for incomplete lines: 1.05 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 0.75 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 1.18 seconds. Estimated time for incomplete lines: 1.18 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "âœ… Evaluation completed with dataset uploaded to AI Foundry\n",
      "ðŸ“ Dataset 'evaluation_dataset_756063988765806918' is now visible in AI Foundry portal\n",
      "ðŸ”® When evaluation API is available, it will use this uploaded dataset\n",
      "ðŸ“Š COMPARISON RESULTS:\n",
      "========================================\n",
      "ðŸ¢ Comparison dataset uploaded to AI Foundry\n",
      "   ðŸ“ Dataset ID: azureai://accounts/aifoundry825233136833-resource/projects/aifoundry825233136833/data/evaluation_dataset_756063988765806918/versions/1.0\n",
      "\n",
      "Case 1: High - Direct, accurate, relevant\n",
      "  Scores:\n",
      "    â€¢ Relevance: 5.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 4.0/5.0\n",
      "    â€¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "Case 2: Low - Doesn't answer the question\n",
      "  Scores:\n",
      "    â€¢ Relevance: 2.0/5.0\n",
      "    â€¢ Coherence: 2.0/5.0\n",
      "    â€¢ Fluency: 3.0/5.0\n",
      "    â€¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "Case 3: Medium - Correct but adds irrelevant info\n",
      "  Scores:\n",
      "    â€¢ Relevance: 5.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 3.0/5.0\n",
      "    â€¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "ðŸ’¡ Notice how the scores reflect the expected quality differences!\n",
      "ðŸ¢ Both evaluation datasets are now available in AI Foundry portal\n"
     ]
    }
   ],
   "source": [
    "# Execute comparative analysis if assessment tools are available\n",
    "if evaluators and len(evaluators) > 0:\n",
    "    print(\"ðŸ” Executing comparative quality analysis...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Run business scenario comparison\n",
    "        comparison_results = foundry_runner.run_evaluation(\n",
    "            data=test_cases,\n",
    "            evaluators=evaluators,\n",
    "            run_name=\"Business Response Quality Analysis\",\n",
    "            description=\"Comparing service quality levels for customer interactions\"\n",
    "        )\n",
    "        \n",
    "        print(\"ðŸ“ˆ COMPARATIVE ANALYSIS RESULTS:\")\n",
    "        print(\"=\" * 45)\n",
    "        \n",
    "        if 'rows' in comparison_results:\n",
    "            for i, (case, row) in enumerate(zip(test_cases, comparison_results['rows'])):\n",
    "                print(f\"\\nScenario {i+1}: {case['expected_quality']}\")\n",
    "                \n",
    "                # Extract business metrics\n",
    "                scores = {}\n",
    "                for key, value in row.items():\n",
    "                    if key.startswith('outputs.') and key.endswith('.relevance'):\n",
    "                        scores['Customer Relevance'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.coherence'):\n",
    "                        scores['Communication Clarity'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.fluency'):\n",
    "                        scores['Professional Standard'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.groundedness'):\n",
    "                        scores['Information Accuracy'] = value\n",
    "\n",
    "                print(\"  Business Impact Scores:\")\n",
    "                for metric, score in scores.items():\n",
    "                    status = \"âœ…\" if score >= 4.0 else \"âš ï¸\" if score >= 3.0 else \"ðŸ”´\"\n",
    "                    print(f\"    {status} {metric}: {score:.1f}/5.0\")\n",
    "                print(\"-\" * 35)\n",
    "        \n",
    "        print(\"\\nðŸ’¼ Business Insight: Quality variations directly correlate with customer satisfaction metrics\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Analysis unavailable: {e}\")\n",
    "        print(\"Concept validated: Response quality directly impacts business outcomes\")\n",
    "else:\n",
    "    print(\"ðŸ“Š Quality assessment framework demonstrates how response variations impact business metrics\")\n",
    "    print(\"Higher quality responses lead to better customer satisfaction and operational efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary & Strategic Recommendations\n",
    "\n",
    "### ðŸ“Š Key Business Outcomes Achieved\n",
    "\n",
    "1. **Quality Assurance Framework**\n",
    "   - Systematic measurement of AI response quality\n",
    "   - Proactive identification of customer experience risks\n",
    "   - Data-driven optimization opportunities\n",
    "\n",
    "2. **Performance Metrics Established**\n",
    "   - **Customer Relevance**: Ensures AI addresses specific customer needs\n",
    "   - **Communication Clarity**: Maintains professional service standards\n",
    "   - **Information Accuracy**: Prevents misinformation and compliance issues\n",
    "   - **Response Consistency**: Delivers reliable customer experience\n",
    "\n",
    "3. **Business Intelligence Capabilities**\n",
    "   - Real-time quality monitoring dashboard\n",
    "   - Comparative analysis for continuous improvement\n",
    "   - ROI measurement for AI investments\n",
    "\n",
    "### ðŸŽ¯ Strategic Recommendations\n",
    "\n",
    "**Immediate Actions**:\n",
    "- Implement quality thresholds for customer-facing AI systems\n",
    "- Establish monitoring protocols for high-impact interactions\n",
    "- Create escalation procedures for below-standard responses\n",
    "\n",
    "**Long-term Strategy**:\n",
    "- Scale assessment framework across all customer touchpoints\n",
    "- Integrate quality metrics with business KPIs\n",
    "- Develop predictive models for customer satisfaction correlation\n",
    "\n",
    "### ðŸ’¼ Business Value Proposition\n",
    "\n",
    "This quality assessment framework provides measurable ROI through:\n",
    "- Reduced customer service escalations\n",
    "- Improved customer satisfaction scores\n",
    "- Decreased operational risk exposure\n",
    "- Optimized AI system performance costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Congratulations! You've completed Lab 1: LLM Evaluation Fundamentals\n",
      "\n",
      "ðŸ“š Ready for Lab 2? Let's scale up your evaluation capabilities!\n"
     ]
    }
   ],
   "source": [
    "# Executive Assessment Template - Customize for specific business scenarios\n",
    "executive_test_data = [\n",
    "    {\n",
    "        \"query\": \"CUSTOMER INQUIRY EXAMPLE\",\n",
    "        \"response\": \"AI SYSTEM RESPONSE\",\n",
    "        \"context\": \"BUSINESS CONTEXT OR POLICY REFERENCE\"\n",
    "    }\n",
    "    # Add business-specific scenarios as needed\n",
    "]\n",
    "\n",
    "# Activate to assess custom business scenarios\n",
    "# if evaluators:\n",
    "#     custom_results = evaluate(data=executive_test_data, evaluators=evaluators)\n",
    "#     print_evaluation_insights(custom_results)\n",
    "\n",
    "print(\"âœ… Quality Assessment Framework Successfully Demonstrated\")\n",
    "print(\"ðŸ“ˆ Business intelligence capabilities validated for customer service optimization\")\n",
    "print(\"ðŸŽ¯ Ready to implement enterprise-wide quality monitoring systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”§ Technical Support & System Requirements\n",
    "\n",
    "**Platform Requirements:**\n",
    "- Azure AI services subscription\n",
    "- Appropriate service quotas and permissions\n",
    "- Network connectivity to Azure endpoints\n",
    "\n",
    "**Common Considerations:**\n",
    "\n",
    "1. **Service Dependencies**: \n",
    "   - Verify Azure AI platform connectivity\n",
    "   - Confirm resource deployment status\n",
    "   - Check subscription and quota limitations\n",
    "\n",
    "2. **Performance Optimization**:\n",
    "   - Adjust batch sizes for optimal throughput\n",
    "   - Configure appropriate timeout settings\n",
    "   - Monitor service usage and costs\n",
    "\n",
    "3. **Security & Compliance**:\n",
    "   - Ensure data handling meets regulatory requirements\n",
    "   - Verify appropriate access controls are in place\n",
    "   - Review audit and logging configurations\n",
    "\n",
    "**Enterprise Support:**\n",
    "- Contact Azure support for platform-specific issues\n",
    "- Consult Microsoft documentation for best practices\n",
    "- Engage with Microsoft customer success teams for optimization guidance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluationagentsfoundry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
