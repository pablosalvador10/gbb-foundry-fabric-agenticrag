{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: LLM Evaluation Fundamentals\n",
    "**Duration**: 60 minutes\n",
    "\n",
    "Welcome to the first lab of our LLM Evaluations Workshop! In this lab, you'll learn the fundamentals of evaluating Large Language Model outputs and understand why traditional software testing approaches don't work for LLMs.\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Understand why LLM evaluation is critical for production systems\n",
    "- Learn core evaluation metrics (relevance, coherence, groundedness)\n",
    "- Perform your first evaluation using Azure AI Foundry SDK\n",
    "- Recognize the non-deterministic nature of LLM outputs\n",
    "\n",
    "## ğŸ“‹ Prerequisites\n",
    "- Completed setup verification notebook\n",
    "- Azure OpenAI credentials configured\n",
    "- Understanding of basic programming concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction - The LLM Evaluation Problem (15 min)\n",
    "\n",
    "### Why Traditional Testing Doesn't Work for LLMs\n",
    "\n",
    "Traditional software testing relies on deterministic inputs and outputs:\n",
    "```python\n",
    "# Traditional function - always returns the same output\n",
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "assert add_numbers(2, 3) == 5  # This will always pass\n",
    "```\n",
    "\n",
    "LLMs are different - they're probabilistic and can generate different outputs for the same input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dependencies imported successfully!\n",
      "Available demo prompts: ['factual', 'explanatory', 'creative', 'analytical', 'instruction']\n",
      "ğŸ¢ Azure AI Foundry integration: ENABLED\n",
      "   âœ… Evaluation results will appear in the AI Foundry portal\n"
     ]
    }
   ],
   "source": [
    "# Let's start by importing our dependencies\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the path so we can import shared utilities\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from shared_utils.azure_clients import azure_manager\n",
    "from shared_utils.foundry_evaluation import foundry_runner\n",
    "from lab1_evaluation_fundamentals.utils.lab1_helpers import (\n",
    "    demonstrate_llm_variability, \n",
    "    print_evaluation_insights,\n",
    "    DEMO_PROMPTS\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Dependencies imported successfully!\")\n",
    "print(f\"Available demo prompts: {list(DEMO_PROMPTS.keys())}\")\n",
    "\n",
    "# Check AI Foundry integration status\n",
    "foundry_status = foundry_runner.get_status_info()\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    print(\"ğŸ¢ Azure AI Foundry integration: ENABLED\")\n",
    "    print(\"   âœ… Evaluation results will appear in the AI Foundry portal\")\n",
    "else:\n",
    "    print(\"ğŸ”§ Azure AI Foundry integration: DISABLED\")\n",
    "    print(\"   â„¹ï¸ To enable portal integration, add to .env:\")\n",
    "    print(\"      AZURE_AI_FOUNDRY_PROJECT_NAME, AZURE_AI_FOUNDRY_ENDPOINT, AZURE_AI_FOUNDRY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Same Prompt, Different Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ EXPERIMENT: Demonstrating LLM Non-Determinism\n",
      "We'll ask the same question 3 times and observe the variations...\n",
      "\n",
      "Running the same prompt 3 times to show variability...\n",
      "Prompt: What is the capital of Japan and what is it known for?\n",
      "--------------------------------------------------------------------------------\n",
      "Run 1:\n",
      "The capital of Japan is **Tokyo**.\n",
      "\n",
      "**Tokyo** is known for several things, including:\n",
      "\n",
      "- **Modernity and Technology:** It is renowned for its cutting-edge technology, vibrant cityscape, and innovative architecture.\n",
      "- **Cultural Heritage:** Tokyo blends traditional culture with modern life, featuring historic temples and shrines, such as Senso-ji and Meiji Shrine.\n",
      "- **Cuisine:** The city is famous for its diverse food scene, including sushi, ramen, and countless Michelin-starred restaurants.\n",
      "- **Fashion and Pop Culture:** Tokyo is a global center for fashion, anime, manga, and youth culture, especially in areas like Harajuku and Akihabara.\n",
      "- **Economic Power:** As one of the world's major financial centers\n",
      "--------------------------------------------------------------------------------\n",
      "Run 2:\n",
      "The capital of Japan is **Tokyo**.\n",
      "\n",
      "Tokyo is known for:\n",
      "- Being one of the worldâ€™s most populous and vibrant cities.\n",
      "- Its blend of ultra-modern skyscrapers and historic temples (such as Senso-ji).\n",
      "- Cutting-edge technology, fashion, and pop culture.\n",
      "- Iconic landmarks like the Tokyo Tower, Tokyo Skytree, and the Shibuya Crossing.\n",
      "- Excellent cuisine, including sushi, ramen, and izakaya dining.\n",
      "- Serving as the political, economic, and cultural center of Japan.\n",
      "- Hosting major events, such as the 2020 Summer Olympics (held in 2021 due to the pandemic).\n",
      "--------------------------------------------------------------------------------\n",
      "Run 3:\n",
      "The capital of **Japan** is **Tokyo**.\n",
      "\n",
      "**Tokyo** is known for:\n",
      "\n",
      "- Being one of the worldâ€™s largest and most populous cities.\n",
      "- Its blend of modern skyscrapers, cutting-edge technology, and historical sites like the Meiji Shrine and Senso-ji Temple.\n",
      "- Vibrant districts such as Shibuya (famous for its busy crossing), Shinjuku, and Akihabara (center of otaku culture).\n",
      "- World-class cuisine, including sushi, ramen, and a high concentration of Michelin-starred restaurants.\n",
      "- Hosting major events, including the 1964 and 2020 Summer Olympics.\n",
      "- Being a global center of finance, business, fashion, and pop culture.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ” OBSERVATION:\n",
      "Notice how each response is different, even though we asked the exact same question!\n",
      "This is why we can't use traditional assert statements for LLM testing.\n"
     ]
    }
   ],
   "source": [
    "# Get Azure OpenAI client\n",
    "client = azure_manager.get_openai_client()\n",
    "deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "\n",
    "# Demonstrate LLM variability with a factual question\n",
    "test_prompt = DEMO_PROMPTS['factual']\n",
    "\n",
    "print(\"ğŸ”¬ EXPERIMENT: Demonstrating LLM Non-Determinism\")\n",
    "print(\"We'll ask the same question 3 times and observe the variations...\\n\")\n",
    "\n",
    "responses = demonstrate_llm_variability(\n",
    "    client=client,\n",
    "    deployment_name=deployment_name,\n",
    "    prompt=test_prompt,\n",
    "    num_runs=3\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ” OBSERVATION:\")\n",
    "print(\"Notice how each response is different, even though we asked the exact same question!\")\n",
    "print(\"This is why we can't use traditional assert statements for LLM testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Impact of Poor LLM Performance\n",
    "\n",
    "Poor LLM performance can lead to:\n",
    "- **Customer Dissatisfaction**: Irrelevant or unhelpful responses\n",
    "- **Brand Risk**: Inconsistent or inappropriate content generation\n",
    "- **Compliance Issues**: Incorrect information in regulated industries\n",
    "- **Cost Overruns**: Using expensive models when cheaper ones would suffice\n",
    "- **Security Risks**: Potential for harmful or biased outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Core Concepts - Understanding Evaluation Metrics (15 min)\n",
    "\n",
    "### Quality Metrics\n",
    "\n",
    "#### 1. Relevance\n",
    "- **Definition**: How well does the response address the question?\n",
    "- **Scale**: Typically 1-5 (1 = completely irrelevant, 5 = perfectly relevant)\n",
    "- **Use Case**: Ensuring responses actually answer the user's question\n",
    "\n",
    "#### 2. Coherence\n",
    "- **Definition**: How clear and logically structured is the response?\n",
    "- **Scale**: Typically 1-5 (1 = incoherent, 5 = perfectly coherent)\n",
    "- **Use Case**: Ensuring responses are understandable and well-organized\n",
    "\n",
    "#### 3. Fluency\n",
    "- **Definition**: How natural and grammatically correct is the language?\n",
    "- **Scale**: Typically 1-5 (1 = poor fluency, 5 = perfect fluency)\n",
    "- **Use Case**: Ensuring professional, readable output\n",
    "\n",
    "#### 4. Groundedness\n",
    "- **Definition**: How well is the response supported by the provided context?\n",
    "- **Scale**: Typically 1-5 (1 = not grounded, 5 = fully grounded)\n",
    "- **Use Case**: Preventing hallucination in RAG applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loaded 8 sample Q&A pairs\n",
      "\n",
      "ğŸ” Sample data format:\n",
      "Query: What is the capital of France?\n",
      "Response: The capital of France is Paris.\n",
      "Context: France is a country in Western Europe. Its capital and largest city is Paris, which is also the coun...\n",
      "Ground Truth: Paris\n",
      "\n",
      "ğŸ’¡ Key Points:\n",
      "- Query: The question or prompt given to the LLM\n",
      "- Response: The LLM's answer\n",
      "- Context: Background information provided to the LLM\n",
      "- Ground Truth: The correct or expected answer (for reference)\n"
     ]
    }
   ],
   "source": [
    "# Let's examine our sample data to understand the format\n",
    "from shared_utils.evaluation_helpers import load_evaluation_data\n",
    "\n",
    "# Load our sample Q&A pairs\n",
    "sample_data = load_evaluation_data('data/sample_qa_pairs.jsonl')\n",
    "\n",
    "print(f\"ğŸ“Š Loaded {len(sample_data)} sample Q&A pairs\")\n",
    "print(\"\\nğŸ” Sample data format:\")\n",
    "\n",
    "# Show the first example\n",
    "example = sample_data[0]\n",
    "print(f\"Query: {example['query']}\")\n",
    "print(f\"Response: {example['response']}\")\n",
    "print(f\"Context: {example['context'][:100]}...\")\n",
    "print(f\"Ground Truth: {example['ground_truth']}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Points:\")\n",
    "print(\"- Query: The question or prompt given to the LLM\")\n",
    "print(\"- Response: The LLM's answer\")\n",
    "print(\"- Context: Background information provided to the LLM\")\n",
    "print(\"- Ground Truth: The correct or expected answer (for reference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety and Performance Metrics\n",
    "\n",
    "Beyond quality, we also evaluate:\n",
    "\n",
    "#### Safety Metrics\n",
    "- **Harmful Content**: Detection of potentially harmful outputs\n",
    "- **Bias Detection**: Identifying biased or discriminatory responses\n",
    "- **Toxicity**: Measuring offensive or inappropriate content\n",
    "\n",
    "#### Performance Metrics\n",
    "- **Latency**: Response time\n",
    "- **Token Usage**: Cost implications\n",
    "- **Throughput**: Requests per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Hands-On - Basic Evaluation (25 min)\n",
    "\n",
    "Now let's perform our first LLM evaluation using Azure AI Foundry SDK!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up the Evaluation Environment (with optional AI Foundry integration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š EVALUATION ENVIRONMENT SETUP\n",
      "==================================================\n",
      "ğŸ¢ Azure AI Foundry Integration: âœ… ENABLED\n",
      "   Your evaluation framework is configured for AI Foundry!\n",
      "   ğŸ“± Portal: https://ai.azure.com\n",
      "\n",
      "   â„¹ï¸ Current Status: azure-ai-projects v1.0.0 detected\n",
      "   ğŸ“Š Evaluations run locally with enhanced metadata\n",
      "   ğŸ”® Future versions will automatically sync to portal\n",
      "\n",
      "ğŸ”§ Current execution mode: enabled\n",
      "   Both modes provide identical evaluation capabilities!\n",
      "   ğŸ¯ The workshop works perfectly with or without AI Foundry integration\n"
     ]
    }
   ],
   "source": [
    "# Check AI Foundry integration status\n",
    "foundry_status = foundry_runner.get_status_info()\n",
    "print(\"ğŸ“Š EVALUATION ENVIRONMENT SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if foundry_status['ai_foundry_available']:\n",
    "    print(\"ğŸ¢ Azure AI Foundry Integration: âœ… ENABLED\")\n",
    "    print(\"   Your evaluation framework is configured for AI Foundry!\")\n",
    "    print(\"   ğŸ“± Portal: https://ai.azure.com\")\n",
    "    print(\"\")\n",
    "    print(\"   â„¹ï¸ Current Status: azure-ai-projects v1.0.0 detected\")\n",
    "    print(\"   ğŸ“Š Evaluations run locally with enhanced metadata\")\n",
    "    print(\"   ğŸ”® Future versions will automatically sync to portal\")\n",
    "else:\n",
    "    print(\"ğŸ”§ Azure AI Foundry Integration: âŒ DISABLED\")\n",
    "    print(\"   Evaluations will run locally (fully functional)\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ’¡ To enable AI Foundry integration, add to your .env file:\")\n",
    "    print(\"   AZURE_AI_FOUNDRY_PROJECT_NAME=your-project-name\")\n",
    "    print(\"   AZURE_AI_FOUNDRY_ENDPOINT=https://your-project.cognitiveservices.azure.com/\")\n",
    "    print(\"   AZURE_AI_FOUNDRY_API_KEY=your-api-key\")\n",
    "    print(\"\")\n",
    "    print(\"ğŸ“š How to get these values:\")\n",
    "    print(\"   1. Go to https://ai.azure.com\")\n",
    "    print(\"   2. Create or select an AI Foundry project\")\n",
    "    print(\"   3. Go to Project Settings â†’ Keys and Endpoints\")\n",
    "    print(\"   4. Copy the endpoint URL and primary key\")\n",
    "\n",
    "print(f\"\\nğŸ”§ Current execution mode: {foundry_status['portal_integration']}\")\n",
    "print(\"   Both modes provide identical evaluation capabilities!\")\n",
    "print(\"   ğŸ¯ The workshop works perfectly with or without AI Foundry integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation environment set up!\n",
      "Model config: gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "# Import evaluation components\n",
    "from azure.ai.evaluation import evaluate, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, GroundednessEvaluator\n",
    "\n",
    "# Get model configuration for evaluators\n",
    "model_config = azure_manager.get_model_config()\n",
    "\n",
    "print(\"âœ… Evaluation environment set up!\")\n",
    "print(f\"Model config: {model_config['azure_deployment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Creating evaluators...\n",
      "âœ… Relevance evaluator created\n",
      "âœ… Coherence evaluator created\n",
      "âœ… Fluency evaluator created\n",
      "âœ… Groundedness evaluator created\n",
      "\n",
      "ğŸ“Š Successfully created 4 evaluators: ['relevance', 'coherence', 'fluency', 'groundedness']\n"
     ]
    }
   ],
   "source": [
    "# Create individual evaluators\n",
    "print(\"ğŸ”§ Creating evaluators...\")\n",
    "\n",
    "try:\n",
    "    relevance_evaluator = RelevanceEvaluator(model_config=model_config)\n",
    "    print(\"âœ… Relevance evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create relevance evaluator: {e}\")\n",
    "    relevance_evaluator = None\n",
    "\n",
    "try:\n",
    "    coherence_evaluator = CoherenceEvaluator(model_config=model_config)\n",
    "    print(\"âœ… Coherence evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create coherence evaluator: {e}\")\n",
    "    coherence_evaluator = None\n",
    "\n",
    "try:\n",
    "    fluency_evaluator = FluencyEvaluator(model_config=model_config)\n",
    "    print(\"âœ… Fluency evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create fluency evaluator: {e}\")\n",
    "    fluency_evaluator = None\n",
    "\n",
    "try:\n",
    "    groundedness_evaluator = GroundednessEvaluator(model_config=model_config)\n",
    "    print(\"âœ… Groundedness evaluator created\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to create groundedness evaluator: {e}\")\n",
    "    groundedness_evaluator = None\n",
    "\n",
    "# Create evaluators dictionary (only include successful ones)\n",
    "evaluators = {}\n",
    "if relevance_evaluator:\n",
    "    evaluators[\"relevance\"] = relevance_evaluator\n",
    "if coherence_evaluator:\n",
    "    evaluators[\"coherence\"] = coherence_evaluator\n",
    "if fluency_evaluator:\n",
    "    evaluators[\"fluency\"] = fluency_evaluator\n",
    "if groundedness_evaluator:\n",
    "    evaluators[\"groundedness\"] = groundedness_evaluator\n",
    "\n",
    "print(f\"\\nğŸ“Š Successfully created {len(evaluators)} evaluators: {list(evaluators.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Running evaluation on 3 examples...\n",
      "\n",
      "Example 1:\n",
      "  Query: What is the capital of France?\n",
      "  Response: The capital of France is Paris....\n",
      "  Has context: Yes\n",
      "\n",
      "Example 2:\n",
      "  Query: How do you calculate the area of a circle?\n",
      "  Response: The area of a circle is calculated using the formula A = Ï€rÂ², where r is the radius of the circle....\n",
      "  Has context: Yes\n",
      "\n",
      "Example 3:\n",
      "  Query: What is machine learning?\n",
      "  Response: Machine learning is a subset of artificial intelligence that enables computers to learn and make dec...\n",
      "  Has context: Yes\n"
     ]
    }
   ],
   "source": [
    "# Take a small subset for our first evaluation\n",
    "evaluation_data = sample_data[:3]  # Use first 3 examples\n",
    "\n",
    "print(f\"ğŸ”¬ Running evaluation on {len(evaluation_data)} examples...\")\n",
    "\n",
    "# Show what we're evaluating\n",
    "for i, item in enumerate(evaluation_data):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Query: {item['query']}\")\n",
    "    print(f\"  Response: {item['response'][:100]}...\")\n",
    "    print(f\"  Has context: {'Yes' if item.get('context') else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Running evaluation...\n",
      "This may take a few minutes as we call the Azure OpenAI service for each metric...\n",
      "âŒ Evaluation failed: (UserError) Unable to load data from './data/lab1_basic_evaluation.json'. Supported formats are JSONL and CSV. Detailed error: Expected object or value.\n",
      "\n",
      "ğŸ”§ Troubleshooting tips:\n",
      "1. Check your Azure OpenAI credentials in .env\n",
      "2. Verify your deployment name is correct\n",
      "3. Ensure you have quota available in your Azure subscription\n",
      "4. Check if your endpoint is accessible\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.evaluation import evaluate, AzureAIProject\n",
    "\n",
    "# Run the evaluation (with optional Azure AI Foundry integration)\n",
    "print(\"ğŸš€ Running evaluation...\")\n",
    "print(\"This may take a few minutes as we call the Azure OpenAI service for each metric...\")\n",
    "\n",
    "try:\n",
    "    client = AIProjectClient(\n",
    "        endpoint=\"https://aifoundry825233136833-resource.services.ai.azure.com/api/projects/aifoundry825233136833\",\n",
    "        credential=DefaultAzureCredential()\n",
    "    )\n",
    "\n",
    "    # Use the enhanced foundry evaluation runner\n",
    "    # results = foundry_runner.run_evaluation(\n",
    "    #     data=evaluation_data,\n",
    "    #     evaluators=evaluators,\n",
    "    #     run_name=f\"Lab 1 Basic Evaluation - {len(evaluation_data)} items\",\n",
    "    #     description=f\"Foundational evaluation with {len(evaluators)} quality metrics\"\n",
    "    # )\n",
    "    proj = AzureAIProject(\n",
    "        subscription_id=os.environ[\"AZURE_SUBSCRIPTION_ID\"],\n",
    "        resource_group_name=os.environ[\"AZURE_RESOURCE_GROUP_NAME\"],\n",
    "        project_name=os.environ[\"AZURE_AI_FOUNDRY_PROJECT_NAME\"],\n",
    "    )\n",
    "    results = evaluate(\n",
    "        data='./data/sample_qa_pairs.jsonl',\n",
    "        evaluators=evaluators,\n",
    "        project=proj,\n",
    "    )\n",
    "    print(\"âœ… Evaluation completed successfully!\")\n",
    "    \n",
    "    # # Show execution method and results\n",
    "    # execution_method = results.get('_execution_method', 'unknown')\n",
    "    \n",
    "    # if execution_method == 'azure_ai_foundry_hybrid':\n",
    "    #     print(\"ğŸ¢ Azure AI Foundry Integration: ACTIVE\")\n",
    "    #     print(f\"   ğŸ“ Dataset uploaded to AI Foundry portal\")\n",
    "    #     print(f\"   ğŸ“Š Dataset ID: {results.get('_dataset_id', 'N/A')}\")\n",
    "    #     print(f\"   ğŸ“ Dataset name: {results.get('_dataset_name', 'N/A')}\")\n",
    "    #     print(f\"   ğŸ‘€ View at: https://ai.azure.com\")\n",
    "    #     print(f\"   ğŸ’¡ {results.get('_note', 'N/A')}\")\n",
    "    #     if 'metrics' in results:\n",
    "    #         print(\"ğŸ“ˆ Evaluation metrics calculated successfully\")\n",
    "            \n",
    "    # elif execution_method == 'azure_ai_foundry_ready':\n",
    "    #     print(\"ğŸ¢ Evaluation completed with AI Foundry integration prepared\")\n",
    "    #     print(f\"   ğŸ“Š Run name: {results.get('_run_name', 'N/A')}\")\n",
    "    #     print(f\"   ğŸ“ Description: {results.get('_description', 'N/A')}\")\n",
    "    #     print(f\"   ğŸ”® Note: {results.get('_note', 'N/A')}\")\n",
    "    #     if 'metrics' in results:\n",
    "    #         print(\"ğŸ“ˆ Metrics calculated successfully\")\n",
    "            \n",
    "    # elif execution_method == 'azure_ai_foundry':\n",
    "    #     print(\"ğŸ¢ Results submitted to Azure AI Foundry portal\")\n",
    "    #     print(f\"   ğŸ“Š Evaluation ID: {results.get('evaluation_id', 'N/A')}\")\n",
    "    #     print(f\"   ğŸ“ Dataset ID: {results.get('dataset_id', 'N/A')}\")\n",
    "    #     print(f\"   ğŸ‘€ View results at: {results.get('portal_url', 'https://ai.azure.com')}\")\n",
    "    #     print(\"   â³ Note: Results may take a few minutes to appear in the portal\")\n",
    "        \n",
    "    # else:\n",
    "    #     print(\"ğŸ”§ Results processed locally\")\n",
    "    #     print(f\"ğŸ“Š Evaluated {len(evaluation_data)} examples\")\n",
    "    #     if 'metrics' in results:\n",
    "    #         print(\"ğŸ“ˆ Metrics calculated successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Evaluation failed: {e}\")\n",
    "    print(\"\\nğŸ”§ Troubleshooting tips:\")\n",
    "    print(\"1. Check your Azure OpenAI credentials in .env\")\n",
    "    print(\"2. Verify your deployment name is correct\")\n",
    "    print(\"3. Ensure you have quota available in your Azure subscription\")\n",
    "    print(\"4. Check if your endpoint is accessible\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Interpret the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.30.1\n",
      "ğŸ“ Loading results from data/lab1_basic_evaluation.json\n",
      "âœ… Results loaded successfully!\n",
      "ğŸ” EVALUATION INSIGHTS\n",
      "==================================================\n",
      "ğŸ“Š Overall Metrics:\n",
      "  â€¢ relevance.relevance: 4.000\n",
      "    â†’ Excellent - Response directly addresses the question\n",
      "  â€¢ relevance.gpt_relevance: 4.000\n",
      "    â†’ Excellent - Response directly addresses the question\n",
      "  â€¢ relevance.relevance_threshold: 3.000\n",
      "    â†’ Good - Response is mostly relevant with minor issues\n",
      "  â€¢ coherence.coherence: 4.000\n",
      "    â†’ Excellent - Response is very clear and well-structured\n",
      "  â€¢ coherence.gpt_coherence: 4.000\n",
      "    â†’ Excellent - Response is very clear and well-structured\n",
      "  â€¢ coherence.coherence_threshold: 3.000\n",
      "    â†’ Good - Response is mostly coherent with good flow\n",
      "  â€¢ fluency.fluency: 3.667\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ fluency.gpt_fluency: 3.667\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ fluency.fluency_threshold: 3.000\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ groundedness.groundedness: 4.000\n",
      "    â†’ Excellent - Response is fully supported by the provided context\n",
      "  â€¢ groundedness.gpt_groundedness: 4.000\n",
      "    â†’ Excellent - Response is fully supported by the provided context\n",
      "  â€¢ groundedness.groundedness_threshold: 3.000\n",
      "    â†’ Good - Response is mostly grounded in the context\n",
      "  â€¢ relevance.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response doesn't adequately address the question\n",
      "  â€¢ coherence.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response lacks clear structure or logical flow\n",
      "  â€¢ fluency.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response has significant language or grammar problems\n",
      "  â€¢ groundedness.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response contains claims not supported by the context\n",
      "\n",
      "ğŸ“ˆ Individual Results (showing first 3 of 3):\n",
      "\n",
      "  Result 1:\n",
      "\n",
      "  Result 2:\n",
      "\n",
      "  Result 3:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import ipykernel\n",
    "\n",
    "ipykernel.__version__\n",
    "print(ipykernel.__version__)\n",
    "results_file = \"data/lab1_basic_evaluation.json\"\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    print(f\"ğŸ“ Loading results from {results_file}\")\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(\"âœ… Results loaded successfully!\")\n",
    "\n",
    "    # Use our helper function to display insights\n",
    "    print_evaluation_insights(results)\n",
    "\n",
    "else:\n",
    "    print(f\"âš ï¸ Results file not found at {results_file}\")\n",
    "    print(\"Make sure you've run the evaluation in the previous step first.\")\n",
    "    print(\"If evaluation failed, that's okay - the key learning is understanding the process.\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Your Results\n",
    "\n",
    "Let's break down what these numbers mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š UNDERSTANDING YOUR EVALUATION RESULTS\n",
      "==================================================\n",
      "\n",
      "ğŸ¯ Overall Average Scores:\n",
      "\n",
      "RELEVANCE: 4.000\n",
      "  ğŸ“ˆ EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "COHERENCE: 4.000\n",
      "  ğŸ“ˆ EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "FLUENCY: 3.667\n",
      "  ğŸ“Š GOOD - Solid performance with room for improvement\n",
      "\n",
      "GROUNDEDNESS: 4.000\n",
      "  ğŸ“ˆ EXCELLENT - Your LLM responses are performing very well!\n",
      "\n",
      "ğŸ“‹ Individual Results (3 examples):\n",
      "========================================\n",
      "\n",
      "Example 1:\n",
      "  Query: What is the capital of France?\n",
      "  Scores:\n",
      "    â€¢ Relevance: 4.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 3.0/5.0\n",
      "    â€¢ Groundedness: 4.0/5.0\n",
      "  Key Insights:\n",
      "    â€¢ Relevance: The response directly and accurately answers the query by stating that Paris is ...\n",
      "    â€¢ Coherence: The response is fully coherent, directly answers the question, and is logically ...\n",
      "\n",
      "Example 2:\n",
      "  Query: How do you calculate the area of a circle?\n",
      "  Scores:\n",
      "    â€¢ Relevance: 4.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 4.0/5.0\n",
      "    â€¢ Groundedness: 5.0/5.0\n",
      "  Key Insights:\n",
      "    â€¢ Relevance: The response directly and accurately provides the formula for calculating the ar...\n",
      "    â€¢ Coherence: The response is coherent, logically organized, and clearly explains how to calcu...\n",
      "\n",
      "Example 3:\n",
      "  Query: What is machine learning?\n",
      "  Scores:\n",
      "    â€¢ Relevance: 4.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 4.0/5.0\n",
      "    â€¢ Groundedness: 3.0/5.0\n",
      "  Key Insights:\n",
      "    â€¢ Relevance: The response directly and accurately defines machine learning, specifying its re...\n",
      "    â€¢ Coherence: The response is coherent, logically organized, and clearly explains what machine...\n",
      "\n",
      "ğŸ’¡ Key Insights:\n",
      "â€¢ Scores are on a 1-5 scale (5 being the best)\n",
      "â€¢ Consistency across metrics indicates well-balanced responses\n",
      "â€¢ Low groundedness might indicate hallucination issues\n",
      "â€¢ Low relevance suggests the model isn't understanding the query well\n"
     ]
    }
   ],
   "source": [
    "if results and 'metrics' in results:\n",
    "    print(\"ğŸ“š UNDERSTANDING YOUR EVALUATION RESULTS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    metrics = results['metrics']\n",
    "\n",
    "    print(\"\\nğŸ¯ Overall Average Scores:\")\n",
    "\n",
    "    # Extract the main metric scores (not the thresholds or binary aggregates)\n",
    "    main_metrics = {}\n",
    "    for metric_name, score in metrics.items():\n",
    "        if not any(x in metric_name for x in ['threshold', 'binary_aggregate', 'gpt_']):\n",
    "            # Get the base metric name (e.g., 'relevance' from 'relevance.relevance')\n",
    "            base_name = metric_name.split('.')[0]\n",
    "            if base_name not in main_metrics:\n",
    "                main_metrics[base_name] = score\n",
    "\n",
    "    for metric_name, score in main_metrics.items():\n",
    "        print(f\"\\n{metric_name.upper()}: {score:.3f}\")\n",
    "\n",
    "        if score >= 4.0:\n",
    "            print(\"  ğŸ“ˆ EXCELLENT - Your LLM responses are performing very well!\")\n",
    "        elif score >= 3.0:\n",
    "            print(\"  ğŸ“Š GOOD - Solid performance with room for improvement\")\n",
    "        elif score >= 2.0:\n",
    "            print(\"  ğŸ“‰ FAIR - Some issues that should be addressed\")\n",
    "        else:\n",
    "            print(\"  ğŸš¨ POOR - Significant improvement needed\")\n",
    "\n",
    "    # Show individual results\n",
    "    if 'rows' in results:\n",
    "        print(f\"\\nğŸ“‹ Individual Results ({len(results['rows'])} examples):\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        for i, row in enumerate(results['rows']):\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "\n",
    "            # Show the query\n",
    "            query = row.get('inputs.query', 'Unknown query')\n",
    "            print(f\"  Query: {query}\")\n",
    "\n",
    "            # Show the scores for this example\n",
    "            scores = {}\n",
    "            for key, value in row.items():\n",
    "                if key.startswith('outputs.') and key.endswith('.relevance'):\n",
    "                    scores['Relevance'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.coherence'):\n",
    "                    scores['Coherence'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.fluency'):\n",
    "                    scores['Fluency'] = value\n",
    "                elif key.startswith('outputs.') and key.endswith('.groundedness'):\n",
    "                    scores['Groundedness'] = value\n",
    "\n",
    "            print(\"  Scores:\")\n",
    "            for metric, score in scores.items():\n",
    "                print(f\"    â€¢ {metric}: {score:.1f}/5.0\")\n",
    "\n",
    "            # Show the reasoning if available\n",
    "            reasons = {}\n",
    "            for key, value in row.items():\n",
    "                if '_reason' in key:\n",
    "                    metric_name = key.split('.')[1].title()\n",
    "                    reasons[metric_name] = value\n",
    "\n",
    "            if reasons:\n",
    "                print(\"  Key Insights:\")\n",
    "                for metric, reason in list(reasons.items())[:2]:  # Show first 2 to avoid clutter\n",
    "                    print(f\"    â€¢ {metric}: {reason[:80]}...\")\n",
    "\n",
    "    print(\"\\nğŸ’¡ Key Insights:\")\n",
    "    print(\"â€¢ Scores are on a 1-5 scale (5 being the best)\")\n",
    "    print(\"â€¢ Consistency across metrics indicates well-balanced responses\")\n",
    "    print(\"â€¢ Low groundedness might indicate hallucination issues\")\n",
    "    print(\"â€¢ Low relevance suggests the model isn't understanding the query well\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nğŸ“š CONCEPTUAL UNDERSTANDING\")\n",
    "    print(\"Even without live results, you now understand:\")\n",
    "    print(\"âœ… How to set up Azure AI evaluation frameworks\")\n",
    "    print(\"âœ… The key metrics for evaluating LLM responses\")\n",
    "    print(\"âœ… How to interpret evaluation scores\")\n",
    "    print(\"âœ… Why evaluation is critical for production LLM systems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” EVALUATION INSIGHTS\n",
      "==================================================\n",
      "ğŸ“Š Overall Metrics:\n",
      "  â€¢ relevance.relevance: 4.000\n",
      "    â†’ Excellent - Response directly addresses the question\n",
      "  â€¢ relevance.gpt_relevance: 4.000\n",
      "    â†’ Excellent - Response directly addresses the question\n",
      "  â€¢ relevance.relevance_threshold: 3.000\n",
      "    â†’ Good - Response is mostly relevant with minor issues\n",
      "  â€¢ coherence.coherence: 4.000\n",
      "    â†’ Excellent - Response is very clear and well-structured\n",
      "  â€¢ coherence.gpt_coherence: 4.000\n",
      "    â†’ Excellent - Response is very clear and well-structured\n",
      "  â€¢ coherence.coherence_threshold: 3.000\n",
      "    â†’ Good - Response is mostly coherent with good flow\n",
      "  â€¢ fluency.fluency: 3.667\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ fluency.gpt_fluency: 3.667\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ fluency.fluency_threshold: 3.000\n",
      "    â†’ Good - Response flows well with minor language issues\n",
      "  â€¢ groundedness.groundedness: 4.000\n",
      "    â†’ Excellent - Response is fully supported by the provided context\n",
      "  â€¢ groundedness.gpt_groundedness: 4.000\n",
      "    â†’ Excellent - Response is fully supported by the provided context\n",
      "  â€¢ groundedness.groundedness_threshold: 3.000\n",
      "    â†’ Good - Response is mostly grounded in the context\n",
      "  â€¢ relevance.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response doesn't adequately address the question\n",
      "  â€¢ coherence.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response lacks clear structure or logical flow\n",
      "  â€¢ fluency.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response has significant language or grammar problems\n",
      "  â€¢ groundedness.binary_aggregate: 1.000\n",
      "    â†’ Poor - Response contains claims not supported by the context\n",
      "\n",
      "ğŸ“ˆ Individual Results (showing first 3 of 3):\n",
      "\n",
      "  Result 1:\n",
      "\n",
      "  Result 2:\n",
      "\n",
      "  Result 3:\n",
      "âœ… Results saved to: /Users/jinle/Repos/_Demos/llm-evaluations-workshop/lab1_evaluation_fundamentals/utils/../data/lab1_basic_evaluation.json\n"
     ]
    }
   ],
   "source": [
    "if results:\n",
    "    # Use our helper function to display insights\n",
    "    print_evaluation_insights(results)\n",
    "    \n",
    "    # Save results for later analysis\n",
    "    from lab1_evaluation_fundamentals.utils.lab1_helpers import save_lab1_results\n",
    "    save_lab1_results(results, \"lab1_basic_evaluation.json\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No results to display - evaluation may have failed.\")\n",
    "    print(\"Don't worry! This is common in workshop environments.\")\n",
    "    print(\"The key learning is understanding the evaluation process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Exploring Different Types of Responses (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª EXPERIMENT: Comparing Response Quality\n",
      "Let's see how different response qualities are scored...\n",
      "\n",
      "Test Case 1: High - Direct, accurate, relevant\n",
      "Response: The capital of France is Paris. Paris is located in the north-central part of France and is the country's largest city.\n",
      "------------------------------------------------------------\n",
      "Test Case 2: Low - Doesn't answer the question\n",
      "Response: Well, France has many beautiful cities, and I think you might be interested in learning about French culture and cuisine. The Eiffel Tower is very famous.\n",
      "------------------------------------------------------------\n",
      "Test Case 3: Medium - Correct but adds irrelevant info\n",
      "Response: The capital is Paris and also France has a population of 67 million people living in cities like Lyon and Marseille which are also important economic centers.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Let's create some intentionally different quality responses to see how evaluation works\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"response\": \"The capital of France is Paris. Paris is located in the north-central part of France and is the country's largest city.\",\n",
    "        \"context\": \"France is a country in Western Europe. Its capital and largest city is Paris.\",\n",
    "        \"expected_quality\": \"High - Direct, accurate, relevant\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"response\": \"Well, France has many beautiful cities, and I think you might be interested in learning about French culture and cuisine. The Eiffel Tower is very famous.\",\n",
    "        \"context\": \"France is a country in Western Europe. Its capital and largest city is Paris.\",\n",
    "        \"expected_quality\": \"Low - Doesn't answer the question\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"response\": \"The capital is Paris and also France has a population of 67 million people living in cities like Lyon and Marseille which are also important economic centers.\",\n",
    "        \"context\": \"France is a country in Western Europe. Its capital and largest city is Paris.\",\n",
    "        \"expected_quality\": \"Medium - Correct but adds irrelevant info\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª EXPERIMENT: Comparing Response Quality\")\n",
    "print(\"Let's see how different response qualities are scored...\\n\")\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    print(f\"Test Case {i+1}: {case['expected_quality']}\")\n",
    "    print(f\"Response: {case['response']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Evaluating different response qualities...\n",
      "\n",
      "ğŸ¢ Running evaluation through Azure AI Foundry...\n",
      "   âœ… Results will appear in the AI Foundry portal\n",
      "ğŸ¢ Running evaluation with Azure AI Foundry integration...\n",
      "ğŸ“¤ Uploading dataset to AI Foundry: evaluation_dataset_756063988765806918\n",
      "âœ… Dataset uploaded successfully: azureai://accounts/aifoundry825233136833-resource/projects/aifoundry825233136833/data/evaluation_dataset_756063988765806918/versions/1.0\n",
      "ğŸ“Š Running evaluation locally (AI Foundry evaluation API not yet available)\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 1.27 seconds. Estimated time for incomplete lines: 2.54 seconds.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 0.69 seconds. Estimated time for incomplete lines: 0.69 seconds.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6251540480 execution.bulk     INFO     Average execution time for completed lines: 0.5 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:42 -0500 6285193216 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:42 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 1.71 seconds. Estimated time for incomplete lines: 3.42 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 1.87 seconds. Estimated time for incomplete lines: 3.74 seconds.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 0.96 seconds. Estimated time for incomplete lines: 0.96 seconds.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6285193216 execution.bulk     INFO     Average execution time for completed lines: 0.66 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 1.97 seconds. Estimated time for incomplete lines: 3.94 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 1.05 seconds. Estimated time for incomplete lines: 1.05 seconds.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6302019584 execution.bulk     INFO     Average execution time for completed lines: 0.75 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 1.18 seconds. Estimated time for incomplete lines: 1.18 seconds.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-08-24 14:48:43 -0500 6268366848 execution.bulk     INFO     Average execution time for completed lines: 0.83 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "âœ… Evaluation completed with dataset uploaded to AI Foundry\n",
      "ğŸ“ Dataset 'evaluation_dataset_756063988765806918' is now visible in AI Foundry portal\n",
      "ğŸ”® When evaluation API is available, it will use this uploaded dataset\n",
      "ğŸ“Š COMPARISON RESULTS:\n",
      "========================================\n",
      "ğŸ¢ Comparison dataset uploaded to AI Foundry\n",
      "   ğŸ“ Dataset ID: azureai://accounts/aifoundry825233136833-resource/projects/aifoundry825233136833/data/evaluation_dataset_756063988765806918/versions/1.0\n",
      "\n",
      "Case 1: High - Direct, accurate, relevant\n",
      "  Scores:\n",
      "    â€¢ Relevance: 5.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 4.0/5.0\n",
      "    â€¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "Case 2: Low - Doesn't answer the question\n",
      "  Scores:\n",
      "    â€¢ Relevance: 2.0/5.0\n",
      "    â€¢ Coherence: 2.0/5.0\n",
      "    â€¢ Fluency: 3.0/5.0\n",
      "    â€¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "Case 3: Medium - Correct but adds irrelevant info\n",
      "  Scores:\n",
      "    â€¢ Relevance: 5.0/5.0\n",
      "    â€¢ Coherence: 4.0/5.0\n",
      "    â€¢ Fluency: 3.0/5.0\n",
      "    â€¢ Groundedness: 3.0/5.0\n",
      "------------------------------\n",
      "\n",
      "ğŸ’¡ Notice how the scores reflect the expected quality differences!\n",
      "ğŸ¢ Both evaluation datasets are now available in AI Foundry portal\n"
     ]
    }
   ],
   "source": [
    "# If we have working evaluators, let's test these cases\n",
    "if evaluators and len(evaluators) > 0:\n",
    "    print(\"ğŸ”¬ Evaluating different response qualities...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Use the enhanced foundry evaluation runner for comparison\n",
    "        comparison_results = foundry_runner.run_evaluation(\n",
    "            data=test_cases,\n",
    "            evaluators=evaluators,\n",
    "            run_name=\"Lab 1 Response Quality Comparison\",\n",
    "            description=\"Comparing high, medium, and low quality responses\"\n",
    "        )\n",
    "        \n",
    "        print(\"ğŸ“Š COMPARISON RESULTS:\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Show execution method for comparison run\n",
    "        execution_method = comparison_results.get('_execution_method', 'unknown')\n",
    "        if execution_method == 'azure_ai_foundry_hybrid':\n",
    "            print(f\"ğŸ¢ Comparison dataset uploaded to AI Foundry\")\n",
    "            print(f\"   ğŸ“ Dataset ID: {comparison_results.get('_dataset_id', 'N/A')}\")\n",
    "        \n",
    "        if 'rows' in comparison_results:\n",
    "            for i, (case, row) in enumerate(zip(test_cases, comparison_results['rows'])):\n",
    "                print(f\"\\nCase {i+1}: {case['expected_quality']}\")\n",
    "                \n",
    "                # Show the scores for this example\n",
    "                scores = {}\n",
    "                for key, value in row.items():\n",
    "                    if key.startswith('outputs.') and key.endswith('.relevance'):\n",
    "                        scores['Relevance'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.coherence'):\n",
    "                        scores['Coherence'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.fluency'):\n",
    "                        scores['Fluency'] = value\n",
    "                    elif key.startswith('outputs.') and key.endswith('.groundedness'):\n",
    "                        scores['Groundedness'] = value\n",
    "\n",
    "                print(\"  Scores:\")\n",
    "                for metric, score in scores.items():\n",
    "                    print(f\"    â€¢ {metric}: {score:.1f}/5.0\")\n",
    "                print(\"-\" * 30)\n",
    "        \n",
    "        print(\"\\nğŸ’¡ Notice how the scores reflect the expected quality differences!\")\n",
    "        \n",
    "        if execution_method in ['azure_ai_foundry_hybrid', 'azure_ai_foundry']:\n",
    "            print(\"ğŸ¢ Both evaluation datasets are now available in AI Foundry portal\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Comparison evaluation failed: {e}\")\n",
    "        print(\"This is common in workshop environments - the concept is what matters!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Evaluators not available for comparison test\")\n",
    "    print(\"But you can imagine how different quality responses would score differently!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Wrap-up and Key Takeaways (5 min)\n",
    "\n",
    "### ğŸ¯ What You've Learned\n",
    "\n",
    "1. **Why LLM Evaluation Matters**\n",
    "   - LLMs are non-deterministic (same input â‰  same output)\n",
    "   - Traditional testing approaches don't work\n",
    "   - Quality directly impacts business outcomes\n",
    "\n",
    "2. **Core Evaluation Metrics**\n",
    "   - **Relevance**: Does it answer the question?\n",
    "   - **Coherence**: Is it clear and logical?\n",
    "   - **Fluency**: Is the language natural?\n",
    "   - **Groundedness**: Is it supported by context?\n",
    "\n",
    "3. **Practical Skills**\n",
    "   - Set up Azure AI evaluation environment\n",
    "   - Create and configure evaluators\n",
    "   - Run evaluations on sample data\n",
    "   - Interpret evaluation results\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "In **Lab 2**, you'll learn how to:\n",
    "- Scale evaluations to larger datasets\n",
    "- Generate synthetic evaluation data\n",
    "- Compare multiple models systematically\n",
    "- Analyze cost vs. quality trade-offs\n",
    "\n",
    "### ğŸ“ Practice Exercise (Optional)\n",
    "\n",
    "Try evaluating some of your own prompts and responses using the techniques you've learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ Congratulations! You've completed Lab 1: LLM Evaluation Fundamentals\n",
      "\n",
      "ğŸ“š Ready for Lab 2? Let's scale up your evaluation capabilities!\n"
     ]
    }
   ],
   "source": [
    "# Practice area - try your own evaluation!\n",
    "your_test_data = [\n",
    "    {\n",
    "        \"query\": \"YOUR QUESTION HERE\",\n",
    "        \"response\": \"YOUR LLM RESPONSE HERE\",\n",
    "        \"context\": \"ANY RELEVANT CONTEXT HERE (optional)\"\n",
    "    }\n",
    "    # Add more test cases as needed\n",
    "]\n",
    "\n",
    "# Uncomment and run to evaluate your own data\n",
    "# if evaluators:\n",
    "#     your_results = evaluate(data=your_test_data, evaluators=evaluators)\n",
    "#     print_evaluation_insights(your_results)\n",
    "\n",
    "print(\"ğŸ‰ Congratulations! You've completed Lab 1: LLM Evaluation Fundamentals\")\n",
    "print(\"\\nğŸ“š Ready for Lab 2? Let's scale up your evaluation capabilities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ†˜ Troubleshooting\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "1. **Import Errors**: \n",
    "   - Run `pip install -r requirements.txt`\n",
    "   - Check that you're in the correct virtual environment\n",
    "\n",
    "2. **Azure Connection Issues**:\n",
    "   - Verify your `.env` file has correct credentials\n",
    "   - Check your Azure OpenAI deployment is active\n",
    "   - Ensure you have sufficient quota\n",
    "\n",
    "3. **Evaluation Failures**:\n",
    "   - This is common in workshop environments\n",
    "   - The learning objective is understanding the process\n",
    "   - Try with smaller datasets or single evaluators\n",
    "\n",
    "4. **Rate Limiting**:\n",
    "   - Add delays between requests\n",
    "   - Use smaller batch sizes\n",
    "   - Check your Azure OpenAI tier limits\n",
    "\n",
    "**Need Help?**\n",
    "- Check the troubleshooting guide: `docs/troubleshooting.md`\n",
    "- Open an issue in the GitHub repository\n",
    "- Review Azure AI documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "zw1klq6xti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current evaluation_data structure:\n",
      "Type: <class 'list'>\n",
      "Length: 3\n",
      "First item keys: ['query', 'response', 'context', 'ground_truth']\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the current values to understand the issue\n",
    "print(\"Current evaluation_data structure:\")\n",
    "if 'evaluation_data' in locals():\n",
    "    print(f\"Type: {type(evaluation_data)}\")\n",
    "    print(f\"Length: {len(evaluation_data)}\")\n",
    "    print(f\"First item keys: {list(evaluation_data[0].keys()) if evaluation_data else 'No data'}\")\n",
    "else:\n",
    "    print(\"evaluation_data not defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tsxbu784q39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… print_evaluation_insights imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Check what's currently imported from lab1_helpers\n",
    "from lab1_evaluation_fundamentals.utils.lab1_helpers import print_evaluation_insights\n",
    "print(\"âœ… print_evaluation_insights imported successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-evals-workshop (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
